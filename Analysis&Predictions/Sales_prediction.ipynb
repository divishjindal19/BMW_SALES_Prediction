{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95de5c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\divis\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC # New import for Support Vector Machine\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier # Kept for final comparison if previous results are available\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1ae90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 18:46:54,078] A new study created in memory with name: no-name-1f9d2fc3-f7cf-4c2b-a6b7-b8fedd638b80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Logistic Regression Baseline Analysis...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Starting Logistic Regression Baseline Analysis...\\n\")\n",
    "\n",
    "# Logistic Regression has fewer hyperparameters, but we can tune C and solver\n",
    "def optimize_logreg(trial):\n",
    "    \"\"\"Optuna objective function for Logistic Regression optimization.\"\"\"\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 1e-3, 10.0, log=True), # Inverse of regularization strength\n",
    "        \"solver\": trial.suggest_categorical(\"solver\", [\"liblinear\", \"lbfgs\"]),\n",
    "        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\"]), # Keep it simple with L2\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "    # Check for compatibility between solver and penalty\n",
    "    if params[\"solver\"] == \"lbfgs\" and params[\"penalty\"] != \"l2\":\n",
    "        # Since we only check \"l2\" for penalty, this check simplifies to:\n",
    "        # if solver is lbfgs, the penalty must be l2 (or none). \n",
    "        # Since we restrict to L2, no pruning is needed here but good practice to keep.\n",
    "        pass\n",
    "\n",
    "    model = LogisticRegression(**params)\n",
    "    acc, f1, prec, rec = cross_val_metrics(model, X, y)\n",
    "    return f1\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_logreg = optuna.create_study(direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9032878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression completed | Accuracy: 0.9000 | F1: 0.9000 | Precision: 0.9000 | Recall: 0.9000\n"
     ]
    }
   ],
   "source": [
    "best_logreg = LogisticRegression(C=1.0, solver=\"liblinear\", random_state=42, n_jobs=-1)\n",
    "# acc_logreg, f1_logreg, prec_logreg, rec_logreg = cross_val_metrics(best_logreg, X, y)\n",
    "acc_logreg, f1_logreg, prec_logreg, rec_logreg = 0.90, 0.90, 0.90, 0.90 # Mock results\n",
    "\n",
    "results_logreg = {\n",
    "    \"Model\": \"Logistic Regression\",\n",
    "    \"Best F1 (Optuna)\": f1_logreg,\n",
    "    \"Accuracy\": acc_logreg,\n",
    "    \"F1 Score\": f1_logreg,\n",
    "    \"Precision\": prec_logreg,\n",
    "    \"Recall\": rec_logreg\n",
    "}\n",
    "\n",
    "print(f\"\\nLogistic Regression completed | Accuracy: {acc_logreg:.4f} | F1: {f1_logreg:.4f} | Precision: {prec_logreg:.4f} | Recall: {rec_logreg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b65e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting Support Vector Machine (SVC) Optimization...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n Starting Support Vector Machine (SVC) Optimization...\\n\")\n",
    "\n",
    "def optimize_svc(trial):\n",
    "    \"\"\"Optuna objective function for SVC optimization.\"\"\"\n",
    "    svc_kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\"])\n",
    "    params = {\n",
    "        \"C\": trial.suggest_float(\"C\", 0.1, 10.0, log=True),\n",
    "        \"kernel\": svc_kernel,\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "    \n",
    "    if svc_kernel == \"rbf\":\n",
    "        params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-4, 1e-1, log=True) # Only for RBF\n",
    "\n",
    "    model = SVC(**params)\n",
    "    # SVC does not support n_jobs > 1 easily, so training can be slow.\n",
    "    acc, f1, prec, rec = cross_val_metrics(model, X, y)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85460191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-22 18:46:54,100] A new study created in memory with name: no-name-3e82f1e6-d41c-404e-9c1c-ad285c5f9af9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVC completed | Accuracy: 0.9100 | F1: 0.9100 | Precision: 0.9100 | Recall: 0.9100\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "study_svc = optuna.create_study(direction=\"maximize\")\n",
    "# NOTE: Need to ensure X and y are defined before running optimize\n",
    "# study_svc.optimize(optimize_svc, n_trials=10, show_progress_bar=True)\n",
    "\n",
    "# Train the best SVC model (using typical defaults for mock)\n",
    "best_svc = SVC(C=1.0, kernel='rbf', random_state=42)\n",
    "# acc_svc, f1_svc, prec_svc, rec_svc = cross_val_metrics(best_svc, X, y)\n",
    "acc_svc, f1_svc, prec_svc, rec_svc = 0.91, 0.91, 0.91, 0.91 # Mock results\n",
    "\n",
    "results_svc = {\n",
    "    \"Model\": \"Support Vector Machine\",\n",
    "    \"Best F1 (Optuna)\": f1_svc,\n",
    "    \"Accuracy\": acc_svc,\n",
    "    \"F1 Score\": f1_svc,\n",
    "    \"Precision\": prec_svc,\n",
    "    \"Recall\": rec_svc\n",
    "}\n",
    "\n",
    "print(f\"\\nSVC completed | Accuracy: {acc_svc:.4f} | F1: {f1_svc:.4f} | Precision: {prec_svc:.4f} | Recall: {rec_svc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907d5e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Selection: The best model based on F1 Score is: LGBMClassifier (F1: 0.9600).\n"
     ]
    }
   ],
   "source": [
    "results_lgbm = {\"Model\": \"LGBMClassifier\", \"F1 Score\": 0.96} \n",
    "best_lgbm = LGBMClassifier() # Mock object\n",
    "\n",
    "all_results = [results_logreg, results_svc, results_lgbm] \n",
    "\n",
    "# Find the model with the highest F1 Score\n",
    "best_model_result = max(all_results, key=lambda x: x[\"F1 Score\"])\n",
    "best_model_name = best_model_result[\"Model\"]\n",
    "\n",
    "if best_model_name == \"Logistic Regression\":\n",
    "    final_model = best_logreg\n",
    "elif best_model_name == \"Support Vector Machine\":\n",
    "    final_model = best_svc\n",
    "elif best_model_name == \"LGBMClassifier\":\n",
    "    # If the tree model (LGBM) is still the objective best, we use it, \n",
    "    # even though the analysis focused on non-tree models.\n",
    "    final_model = best_lgbm \n",
    "\n",
    "print(f\"\\nFinal Selection: The best model based on F1 Score is: {best_model_name} (F1: {best_model_result['F1 Score']:.4f}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e4411f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m joblib.dump(final_model, \u001b[33m\"\u001b[39m\u001b[33mmodels/best_final_model.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m joblib.dump(\u001b[43mscaler\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33mmodels/scaler.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m joblib.dump(le_y, \u001b[33m\"\u001b[39m\u001b[33mmodels/label_encoder.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSaved final artifacts (mocked): models/best_final_model.joblib, models/scaler.joblib, models/label_encoder.joblib\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(final_model, \"models/best_final_model.joblib\")\n",
    "joblib.dump(scaler, \"models/scaler.joblib\")\n",
    "joblib.dump(le_y, \"models/label_encoder.joblib\")\n",
    "\n",
    "print(f\"\\nSaved final artifacts (mocked): models/best_final_model.joblib, models/scaler.joblib, models/label_encoder.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255ef21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
